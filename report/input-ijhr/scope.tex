%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% introduction

Knowledge of object shape is necessary for many manipulation tasks \cite{Bajcsy1989Machine}. The best shape representation depends on the precise task, but there are many generally desirable properties. These include, among others: accuracy, compactness, an intuitive parameterisation, local support, affine invariance, an ability to handle arbitrary topologies, guaranteed continuity, efficient rendering and support for efficient collision detection. Since we are concerned  with shape recovery for arbitrary novel objects, the capacity to represent an arbitrary topology while retaining guaranteed continuity is desirable. Implicitly defined surfaces have these properties.

There are several ways to define an implicitly defined surface, e.g. via algebraic equations, blobby models, or variational surfaces. These classical representations do not include any  measure of shape uncertainty. There is where the work by \cite{Williams2007Gaussian} comes in, introducing the notion of a Gaussian Process implicit surface (GPIS). This is not the only representation to account for uncertainty, but it meets the requirements for a good shape representation in robotics, as discussed above. Sub-section~\ref{sec:gpis} introduces the notation for GPIS.

The surface estimate, when using GPIS, is simply the mean value of a Gaussian Process, which is the $0$-level set of an implicitly defined manifold. Henderson et al. \cite{Henderson1993COMPUTING} provide a way to recover the implicitly defined surface via continuation techniques. That work has been extended in a number of different directions, including one of particular interest for local exploration. The AtlasRRT algorithm is a path planning method for constrained manipulators \cite{Jaillet2013Path}. It combines continuation techniques for surface recovery with rapidly-exploring random trees (RRT) \cite{LaValle2011Motion} for path generation. This combination allows the computation of paths in a constrained configuration space, i.e. a manifold (such as a surface) embedded in an ambient space (such as the workspace). However, the AtlasRRT, as employed to date, makes no use of uncertainty in the manifold. By combining the AtlasRRT algorithm with the concept of uncertainty as modelled with GPIS we can derive a powerful planner for traversing an uncertain surface. This is the central technical contribution of this paper. Sub-section~\ref{sec:atlas-rrt} describes the basic idea behind the AtlasRRT algorithm.

The final ingredient required for bi-manual object exploration is the equipment necessary to simultaneously grasp an object with one hand whilst exploring it with the other. In sub-section~\ref{sec:limitations}, we enumerate the considerations for the hardware that is to execute tactile exploration, as well as possible limitations. Finally, with all these ingredients in mind, we formally define our problem in sub-section~\ref{sec:problem_definition}.

%Active perception enables a robot to reason on how to efficiently acquire more (task-related) information in a noisy environment in order to complete a task at hand.
%The task we pursue in this work is to enable a robot to reason on how to autonomously combine visual and haptic information to identify the shape of household objects that can be successively grasped and manipulated by a human-sized hand.

%Our approach is to formalise the problem of shape representation as a problem of linear regression, in which a Gaussian Process (GP) is employed to learn a probabilistic model of the object. The GP allows us to build a \emph{maximum a posteriori} (MAP) estimation of the object's surface described as an implicit function, $f(\mathbf{x})=0$, (the 0-levelset of the approximating function) as well as to constrain the learnt function on visual and haptic clues in order to converge to the real shape of the object. To enable the robot to select best-next (exploration) actions, we construe the function $f(\mathbf{x})$ as a manifold and we grow an exploration tree, similar to the AtlasRRT described in~\cite{Jaillet2013Path}, which is biased to visit uncertain region of the object's surface.

%This section proceeds as follows. We firstly introduce the general formulation for GPR for implicit surfaces. We then show how to compute first-order quantities from the GP (surface normals) which are essential to locally parametrise a manifold (Atlas). Finally, we present how to build a sample-based exploration on the manifold to visit regions of the object's surface that are more promising to reduce the global model uncertainty.

%\todo[]{Equipment specification and assumptions have been removed. They should be move later when describing the experimental setup.}
% CR the idea is the you specified abstractly what are the hardware requirements this algorithm is supposed to work with, and then in the experiment you say what hardware you use that comply with the requirements


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Background}
%\label{sec:resources}

%---------------------------------------
\subsection{Gaussian Process Implicit Surfaces}
\label{sec:gpis}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\columnwidth]{mugS.eps}
    \caption{Gaussian Implicit Surface, obtained from a mug (top-left corner) and sampled with a box-grid evaluation with fairly high point density. Each point has a predicted target $y_* \sim= 0$ and is colored accordingly to its associated
    predicted variance, from red (high variance) to green (low variance).}
    \label{fig:gpmug}
\end{figure}

A surface embedded in a 3D Cartesian space can be regarded as the $0$-level set of a family of surfaces defined by an implicit function $F(\mathbf{x}, y)=0$, where $F:\mathbb{R}^{3+1}\rightarrow \mathbb{R}$, with coordinates $\mathbf{x}\in \mathbb{R}^3$ and parameter $y\in \mathbb{R}$.
 Under the assumption that the Implicit Function Theorem holds, it can be expressed, at least locally, as $y = f(\mathbf{x})$, with $F(\mathbf{x},f(\mathbf{x}))=0$. The surface of  interest arises when we set $y = 0$, i.e.\ when we define the $0$-level set. The value of $f$ (i.e. $y$) is positive and increasing as we move outwards from the surface ($\nabla F$), and negative and decreasing as we move further inside the object ($-\nabla F$).

A Gaussian process (GP) ``is a collection of random variables, any finite number of which have a joint Gaussian distribution'' \cite[Def. 2.1]{Rasmussen2006Gaussian}. It is completely specified by a mean, $m(\mathbf{x}) = \mathbb{E}[f(\mathbf{x})]$, and a covariance, $k(\mathbf{x},\mathbf{x}') = \mathbb{E}[(f(\mathbf{x}) - m(\mathbf{x}))(f(\mathbf{x}') - m(\mathbf{x}'))]$, function, where $\mathbb{E}(\cdot)$ is the expected value of a real process, such that we can write
\begin{equation}
f(\mathbf{x}) \sim \mathcal{GP}(m(\mathbf{x}), k(\mathbf{x},\mathbf{x}')).
\end{equation}

Now, let $\mathcal{S}$ be a set of tuples $s_i = (\mathbf{x}_i, \sigma_i, y_i)$ with $i=1,\ldots,n$. Here, the $\mathbf{x}_i$ are points in the Cartesian workspace, $\sigma_i$ are corresponding noise parameters of the tactile observations, and $y_i$ is the target value for the implicit function (either -1,0 or +1). The set $S$ constitutes the tactile observations that are used as the training set for the GP. We specify $m(\mathbf{x})=0$ to yield the model
\begin{equation}
\mathbf{y} \sim \mathcal{N}(\mathbf{0}, K(\mathcal{X},\mathcal{X}) + \boldsymbol{\sigma}^\top I \boldsymbol{\sigma}), \label{eq:gp_model}
\end{equation}
where $\mathcal{N}(\cdot, \cdot)$ denotes a normal distribution parameterised by mean and variance; $\mathcal{X}$ corresponds to the inputs from the training set $\mathcal{S}$;  $K(\cdot, \cdot)$ is the covariance matrix formed from  elements $k_{ij} = k(\mathbf{x}_i, \mathbf{x}_j)$, for all pairs of input points $i,j : \mathbf{x}_i, \mathbf{x}_j \in \mathcal{X}$; $\boldsymbol{\sigma}$ is the $n$-dim vector corresponding to the noise of the $i^{th}$ observation\footnote{Therefore, $I$ is an $n \times n$ identity matrix.}; and finally, $\mathbf{y}$ is the $n$-dim vector of target outputs. 

The purpose is to predict a vector of target values, $\mathbf{y}_*$, given test inputs $\mathcal{X}_*$. To achieve this Equation~\ref{eq:gp_model} can be block-expanded \footnote{For simplicity, we drop the arguments of the matrices such that $K = K(\mathcal{X},\mathcal{X})$, $K_* = K(\mathcal{X},\mathcal{X}_*)$ and $K_{**} = K(\mathcal{X}_*,\mathcal{X}_*)$} \cite{Rasmussen2006Gaussian} to give
\begin{equation}
    \begin{bmatrix} \mathbf{y} \\ \mathbf{y}_* \end{bmatrix} \sim
               \mathcal{N}\begin{pmatrix}\mathbf{0}, & \begin{bmatrix} K + \boldsymbol{\sigma}^{T} I \boldsymbol{\sigma} &
                                                 K_* \\
                                                 K_*^\top &
                                                 K_{**} \end{bmatrix}
                           \end{pmatrix},
\end{equation}
%where $\mathcal{X}_*$ is the set of test inputs, and $\mathbf{y}_*$ their corresponding predicted values by the trained GP model, arranged in the same $n$-vector fashion. 

The two predictive equations can then be derived via algebraic manipulation
\begin{alignat}{2}
	\mathbf{y}_* = & K_*^\top [K + \boldsymbol{\sigma}^{T} I \boldsymbol{\sigma})]^{-1}\mathbf{y}, \label{eq:f_prediction}\\
	\mathbb{V}(\mathbf{y}_*) = & K_{**} - K_*^\top[K + \boldsymbol{\sigma}^{T} I \boldsymbol{\sigma})]^{-1} K_*. \label{eq:variance_f}
\end{alignat}
The first is the vector of predicted values of the implicit function, and the second is vector of the variances in those predictions. If we wish to make a prediction for a single test input $\mathbf{x_*}$ we follow \cite{Rasmussen2006Gaussian} in further simplifying the notation for the covariances. In that case there is a vector of covariances between the test point and each of the training inputs, denoted $\mathbf{k}(\mathcal{X},\mathbf{x}_*)$. We will use this later on.

The key choice in using a GP is the choice of kernel for specifying the covariance between two points in the input space. Intuitively if input points $\mathbf{x}_i, \mathbf{x}_j$ are close together then, in order to have a smooth function, they should strongly covary. Conversely, as the distance between input points $\mathbf{x}_i, \mathbf{x}_j$ increases their covariance tends to zero.
%The covariance function specifies a distribution over functions with a notion of nearness among them, and it is a critical ingredient to select the appropriate one to get coherent predictions out of a GP model, a problem sometimes referred to as the \emph{kernel trick}. There is actually a full chapter dedicated to its study by \cite[see Ch. 4]{Rasmussen2006Gaussian}, however, none of them seem adequate for points belonging to an implicitly defined surface. 
We utilise the idea, proposed by Williams \cite{Williams2007Gaussian}, to use the thin-plate kernel 
\begin{equation}
k(r) = 2r^{3} - 3Rr^2 + R^3,
\end{equation}
with $r = \| \mathbf{x} - \mathbf{x}' \|_2$ and $R$ being the largest $r$ in the training set. This training set only consists of points on the object surface. To aid training of the implicit function we therefore extend the training set to be the composition three sets. First, there are points on the surface $\mathcal{S}^0$, with tuples of the form $s_i = (\mathbf{x}_i, \sigma_i, 0)$. Then there are points outside the surface, $\mathcal{S}^+$, with tuples of the form $s_i = (\mathbf{x}_i, 0, +1)$ and finally there are points inside the surface, $\mathcal{S}^-$, with tuples of the form $s_i = (\mathbf{x}_i, 0, -1)$. Thus, the training set $\mathcal{S} = \mathcal{S}^0 \cup \mathcal{S}^+ \cup \mathcal{S}^-$. \footnote{Without loss of generality, but with a slight gain in efficiency and parameter tuning, we can also work in a normalized and offset-free space, using as scale the larger distance and the centroid from the training set. This way, for instance, $R$ becomes a fixed parameter, as well as the $S^+$ and $S^-$ sets, a trick also exploited by \cite{Li2016Dexterous}. Of course, the model exploitation requires a re-scaling and re-centering processing step.}

We are now able to predict the target $y_* = \bar{f}(\mathbf{x}_*)$ and its variance $\mathbb{V}[f(\mathbf{x}_*)]$ for any given test point $\mathbf{x}_*$ in the workspace, %namely $\mathbb{R}^3$ in our case,
given the training set $\mathcal{S}$. To find the implicit surface we need only exhaustively evaluate $y_*$ for each point $\mathbf{x}_*$ in a $3$-dimensional box-grid containing the object. The predicted surface points $\mathbf{x}_*$ are those where $y_* \simeq 0$. 
%that improve the overall variance of the predicted shape, and not with a complete rendering as the box-grid evaluation, or even a marching cube algorithm, that requires time. As an example, 
Fig.~\ref{fig:gpmug} shows an GPIS, for the pictured mug, sampled with a box-grid evaluation. We can use this to find candidate surface points for tactile exploration. 

We must also find the best direction for the finger to approach the surface. A good choice is the predicted surface normal at the candidate point. In our case, the normal is parallel to the gradient of the function. If we consider the posterior mean of the GP given in (Equation~\ref{eq:f_prediction}) for a single test point, we have
\begin{alignat}{2}
\bar{f}(\mathbf{x}_*) = & \mathbf{k}(\mathcal{X},\mathbf{x}_*)^\top [K + \boldsymbol{\sigma}^{T} I \boldsymbol{\sigma})]^{-1}\mathbf{y} \nonumber \\ = & \mathbf{k}(\mathcal{X},\mathbf{x}_*)^\top \boldsymbol{\alpha}.
\end{alignat}
Note that the vector $\boldsymbol{\alpha}$ is constant for a given training set $\mathcal{S}$, whereas the vector $\mathbf{k}(\mathcal{X},\mathbf{x}_*)$ gathers the covariance values between the test point and the training set being the only term depending on the test point. Therefore, the  gradient evaluated at $\mathbf{x}_*$ is
\begin{equation}
 \frac{\partial \bar{f}(\mathbf{x}_*)}{\partial \mathbf{x}_*} = \frac{\partial \mathbf{k}(\mathbf{x}_*,\mathcal{X})}{\partial \mathbf{x}_*} \boldsymbol{\alpha}, \label{eq:gradient_f}
\end{equation}
which boils down to evaluating, for each combination of test and training point, the derivative of the thin-plate covariance function
\begin{alignat}{2}
  \frac{\partial k(r)}{ \partial r} \frac{\partial r}{ \partial \mathbf{x}_*} = & [6r (r - R)] \frac{\mathbf{x}_i - \mathbf{x}_*}{\| \mathbf{x}_i - \mathbf{x}_* \|_2} \nonumber \\ \frac{\partial k_*}{ \partial \mathbf{x}_*} = & 6(r - R) (\mathbf{x}_i - \mathbf{x}_*),
\end{alignat}
for all $i : \mathbf{x}_i \in \mathcal{X}$. Consequently, the normal at the the test point, $\mathbf{n}_*$, is obtained dividing the gradient by its magnitude.

%Let $\mathcal{S}$ be a training dataset of $N$ observations, $\mathcal{S}=\{(\mathbf{x}_i, y_i)|i=1,\dots,N\}$, where $\mathbf{x}_i\in\mathbb{R}^n$ denotes the $i^{th}$ input vectors and $y_i\in\mathbb{R}$ the associated scalar output or target.  We collect the $N$ inputs vectors in a $n\text{ x }N$ matrix $X$ and the output target in a $N\text{ x }1$ vector $\mathbf{y}$, so that we can rewrite the training dataset in a more compact way as $\mathcal{S}=(X,\mathbf{y})$. We are interested in making inference about the mapping function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ between the input vectors and the targets. We assume a linear regression model
%\[
%y_i=f(\mathbf{x}_i)+\epsilon
%\]
%where $\epsilon\thicksim\mathcal{N}(0,\sigma_n^2)$ is an independent identically distributed Gaussian noise with zero-mean and variance $\sigma_n^2$,


%A Gaussian process framework for regression (GPR) is a common choice to approximate $f(\mathbf{x})$, in which the targets $y_i$ are assumed to be drawn from a zero-mean multi-variate Gaussian distribution with a covariance matrix which is a function of the input vectors. Therefore the output distribution can be written as
%\begin{eqnarray}
%\label{eq:gpr}
%(y_1,\dots,y_N|\mathbf{x}_1,\dots,\mathbf{x}_N)\thicksim\mathcal{N}(0,K(X,X)+\sigma_n^2I)
%\end{eqnarray}
%The covariance function expresses somehow the notion of nearness or similarity, for which points in the input space $\mathbb{R}^n$ that are close would likely produce similar outputs. The choice of a kernel is a crucial ingredient for a GP and a vast part of the literature has investigated this problem, sometimes referred as the \emph{kernel trick}. Following previous works on implicit surface estimations we chose the \emph{thin plate} kernel, which is defined as
%\begin{eqnarray}
%\label{eq:thinplate}
%K(\mathbf{x}_i,\mathbf{x}_j)=2\|\mathbf{x}_i-\mathbf{x}_j\|_2^3-3R\|\mathbf{x}_i-\mathbf{x}_j\|_2^2+R^3
%\end{eqnarray}
%where $\|\mathbf{a}\|_2=\sqrt{\mathbf{a}^\top\mathbf{a}}$ and $R=\max(\|\mathbf{x}_i-\mathbf{x}_j\|_2),\forall\mathbf{x}_i,\mathbf{x}_j\in\mathcal{S}$, is the largest pairwise distance between the input vectors in the training dataset $\mathcal{S}$. To improve the prediction performance of the GP with a thin plate kernel is useful to divide the training dataset in three disjointed subsets, namely $\mathcal{S}^0=\{(\mathbf{x}_i,y_i)\in\mathcal{S}|y_i=0\}$ are the inputs labelled as ``on the object's surface'', $\mathcal{S}^+=\{(\mathbf{x}_i,y_i)\in\mathcal{S}|y_i=+1\}$ and $\mathcal{S}^0=\{(\mathbf{x}_i,y_i)\in\mathcal{S}|y_i=-1\}$ are artificial inputs, respectively, laying outside and inside the object's surface. We then can rewrite the training dataset as $\mathcal{S}=\mathcal{S}^0\cup\mathcal{S}^+\cup\mathcal{S}^-$ and its cardinality $N=N^0+N^++N^-$ as the sum of the respective cardinalities.

%Given a new point $\mathbf{x}_*$, it is possible to query the GP to compute the estimate of $y_*=\bar{f}(\mathbf{x}_*)$ with a measure of confidence expressed by its associated variance $\mathbb{V}[y_*]$ by deriving the following equations from Eq.~\ref{eq:gpr}:

%\begin{eqnarray}
%\label{eq:gpr_mu}
%y_*=\mathbf{k}_*^\top(K+\sigma_n^2I)^{-1}\mathbf{y}
%\end{eqnarray}

%\begin{eqnarray}
%\label{eq:gpr_var}
%\mathbb{V}[y_*]=k(\mathbf{x}_*,\mathbf{x}_*)-\mathbf{k}_*^\top(K+\sigma_n^2I)^{-1}\mathbf{k}_*
%\end{eqnarray}
%Notice that we have introduced a more compact notation where, for a single query point $\mathbf{x}_*\in\mathbb{R}^n$, $\mathbf{k}_*=K(\mathbf{x}_*,X)$ is the $N\text{ x }1$ covariance vector between the query point and the training dataset $X$. Similarly, $K=K(X,X)$ is the covariance matrix evaluated on the training dataset and $k(\mathbf{x}_*,\mathbf{x}_*)$ is a scalar value denoting the variance between the query point and itself.

%\subsection{Gaussian process derivative}
%\label{sec:gpr_der}

% The gradient estimation can be computed using a mixed covariance function that takes as argument function values and partial derivatives of that function, that is,
% \begin{equation}
% k\left(f(\mathbf{x}), \frac{\partial f(\mathbf{x})}{\partial\mathbf{x}}\right)=\frac{\partial k(\mathbf{x}, \mathbf{x}')}{\partial\mathbf{x}}.
% \end{equation}



% In the previous section we introduced a general formulation of GPR to describes implicit surfaces in terms of its mean (Eq.~\ref{eq:gpr_mu}) and variance (Eq.~\ref{eq:gpr_var}). We also shown that, given an query input $\mathbf{x}_*$ it is possible to predict its expected target value, $\bar{f}(\mathbf{x}_*)$ as well as its associated expected variance $\mathbb{V}[f(\mathbf{x}_*)]$. As described in~\cite{Rasmussen2006Gaussian}, a GPR can be utilised to make prediction on the gradient of the approximating function, $\mathbb{E}[f'(\mathbf{x})]$. In the case of implicit surface this information is equivalent to make an estimate of the normal surface at the evaluated point $\mathbf{x}_*$.\todo[]{add citation here}

% The gradient estimation can be computed by using a mixed covariance function between function values and partial derivates. Given two input vectors $\mathbf{x}_i$ and $\mathbf{x}_j$ the mixed covariance function can be written as
% $$
% cov(f(\mathbf{x}_i), \frac{\partial f(\mathbf{x}_j)}{\partial\mathbf{x}})=\frac{\partial k(\mathbf{x}_i, \mathbf{x}_j)}{\partial\mathbf{x}_j}
% $$
% which is equivalent to calculate the vector of partial derivatives with respect to $\mathbf{x}_j$ of the original kernel function. The first partial derivate of the thin plate covariance can be written as
% \begin{eqnarray}
% \label{eq:gpr_der}
% \frac{\partial k(\mathbf{x}_i,\mathbf{x}_j)}{\partial\mathbf{x}_j}&=6(\mathbf{x}_i-\mathbf{x}_j)(\|\mathbf{x}_i-\mathbf{x}_j\|_2 + R)
% \end{eqnarray}

% For a single query point $\mathbf{x}_*$, we define
% $$
% \frac{\partial K_*}{\partial\mathbf{x}}=\frac{\partial\mathbf{k}(X,\mathbf{x})}{\partial\mathbf{x}}\bigg|_{\mathbf{x}=\mathbf{x}_*}
% $$
% as the $N\text{ x }n$ mixed covariance matrix between the function values at the training points $X$ and the partial derivatives evaluated at the query point.

% Therefore, similarly to Eq.\ref{eq:gpr_mu}, the expected normal, $\mathbf{n}_*$, on the query input $\mathbf{x}_*$ can be computed by the following equation

% \begin{eqnarray}
% \label{eq:gpr_n}
% \mathbf{n}_*=\mathbb{E}[f'(\mathbf{x})]=\frac{\partial K_*}{\partial\mathbf{x}}^\top(K+\sigma_n^2I)^{-1}\mathbf{y}
% \end{eqnarray}
% where $\mathbf{n}_*$ is a $n\text{ x }1$ vector.

%---------------------------------------
\subsection{Defining an Atlas of an implicitly-defined surface}
\label{sec:atlas-rrt}

%Following the formulation in~\cite{Jaillet2013Path},

How might we represent the implicit surface in such a way that we can easily create paths across it using standard path planning algorithms?  The insight comes from the fact that the surface is simply one example of a more general phenomenon: a smooth manifold embedded in some higher dimensional space. Henderson~\cite{Henderson1993COMPUTING} gave a precise method to model such manifolds via \emph{disks}. Each disk is a tangent space with its centre at a point on the manifold.
%. The disks are further projected onto the surface using a logarithmic mapping. 
If we think about this for our special case. The creation of a model typically starts with an initial point, $\mathbf{x} \in \mathbb{R}^3$, known to be on the surface, $f(\mathbf{x}) = 0$ (or very close and projected onto it), which is the centre of the first disk. This can be thought of as a chart. New adjoinng disks can be created from this disk. The method continues iteratively until all disks are surrounded by neighbours, and this provides a complete coverage of the shape. This defines an atlas comprising many charts. This concept has been widely used, including for obtaining representations of constrained configuration spaces \cite{Porta2014CuikSuite}, such as object surfaces. We now give some details.

First, recall that our implicit surface is defined by the equality constraint $f$ that holds for all points in the set $\mathcal{X_0}$ of points on the object surface
\begin{equation}
\mathcal{X_0} = \{\mathbf{x} \in \mathbb{R}^3 : f(\mathbf{x}) = 0 \}.
\end{equation}
For any point $\mathbf{x}_i \in \mathcal{X}$, we can find its tangent space, i.e. the tangent plane to the surface at $\mathbf{x}_i$. The matrix $\boldsymbol{\Phi}_i$ is the basis of the tangent space for $\mathbf{x}_i$. This therefore defines the mapping of points from this tangent space into $\mathbb{R}^3$.
Matrix $\boldsymbol{\Phi}_i$ satisfies
\begin{equation}
\begin{bmatrix} \nabla f(\mathbf{x}) \\ \boldsymbol{\Phi}_i^\top \end{bmatrix} \boldsymbol{\Phi}_i = \begin{bmatrix}  0 \\ I \end{bmatrix}, \label{eq:tangent_basis}
\end{equation}
where $n=3$ and $k=2$, hence $I$ is the $2\times2$ identity matrix, and $\boldsymbol{\Phi}_i$ is a $3\times2$ matrix. 
%This basis serves as an approximation of the commonly known exponential, $\mathbf{x}_j = \phi_i(\mathbf{u}_i)$, and logarithmic $\mathbf{u}_i = \phi_i^-1(\mathbf{x}_j)$, mappings, that satisfy the condition $\mathbf{x}_i = \phi_i(\mathbf{0})$, where 
Now let $\mathbf{u}$ be the coordinate of a point in the tangent space of $\mathbf{x}_i$. It can be mapped to a point $\mathbf{x}'_i \in \mathbb{R}^3$ as follows
\begin{equation}
  \mathbf{x}'_i = \mathbf{x}_i + \boldsymbol{\Phi}_i \mathbf{u}_i, \label{eq:tangent_approx}
\end{equation}
Next, we find the orthogonal projection of $\mathbf{x}'_i$ onto the object surface, giving $\mathbf{x}_j$. This is achieved by solving the system
\begin{equation}
\begin{cases}
f(\mathbf{x}_j) = 0,
\\
\boldsymbol{\Phi}^\top( \mathbf{x}_j - \mathbf{x}'_i ) = 0,
\end{cases} \label{eq:projection}
\end{equation}
We using a gradient descent like method to solve this. This new point $x_j$ becomes the surface point defining the centre of the next chart.\footnote{The region of a chart is typically bounded using rules about the local curvature and the distance from the tangent space to the manifold. However, since these features are not precisely known in our scenario, we instead employ a slightly different criterion to bound each chart.}

Thus, given a point $\mathbf{x}_i \in \mathcal{X}$, one can build a chart $\mathcal{C}_i$ that allows us to obtain a new point $\mathbf{x}_j \in \mathcal{X}$. Then, this new point can be used to generate a new chart $\mathcal{C}_j$, and so on. In order to avoid the parametrization of areas already covered by other charts, they are intersected according to their validity region, introducing the notion of bounded and unbounded charts, depending on whether they have been intersected from all directions or not. For instance, the initial chart is by definition unbounded. This coordination process yields the concept of an atlas $\mathcal{A}$: a collection of properly coordinated charts, that completely covers the manifold when there are no unbounded charts. Since we already differ on the validity region of each chart, we naturally also differ on their coordination.

Due to our assumptions at the beginning of the section, the manifold $\mathcal{X}$ is smooth everywhere, and we should not expect singularities either. The function, of course, exists for any point in the ambient space as $f:\mathbb{R}^3 \rightarrow \mathbb{R}$, as well as the tangent basis approximation of the exponential and logarithmic mapping, and related concepts.

Up to this point, we are able to compute an atlas $\mathcal{A}$ of an implicitly defined manifold ---again, or surface---, $\mathcal{X}$, given at least a point $\mathbf{x}_i \in \mathcal{X}$ (or very close to it so we can project it using the tangent basis approximation). However, nothing has been said about what would be the preferred direction from the initial chart $\mathcal{C}_i$ to expand. If we are computing the atlas exhaustively, we can choose randomly, since we are expecting to cover it all anyway. However, if one wishes to go from point $\mathbf{x}_i \in \mathcal{X}$ to point $\mathbf{x}_j \in \mathcal{X}$ always being in the manifold, perhaps there is no interest in computing the full atlas, but only the interesting part of the path that connects them. \cite{Jaillet2013Path} successfully combines the RRT technique with this idea of computing collision-free paths of constrained mechanisms. The main difference with our strategy is we configure the RRT for exploration of uncertain regions, that is, the atlas tends to go towards regions of the predicted surface that need to be improved via tactile exploration, which it is actually the goal RRTs were proposed for at first.

%In this work we aim to generate a path along an object surface, described as a manifold over a GP estimation, $\mathcal{F}$, that a robotic finger equipped with a F/T sensor can follow to gather new information about the shape of the object. Such an information will be then integrated in a probabilistic model (GP) in order to refine the manifold. The goal is to iteratively converge to a model of the object's shape in which the shape uncertainty becomes negligible.

%We build on the recent advances on sample-based techniques for asymptotically optimal exploration of a general manifold. Similarly to the work of~\cite{Jaillet2013Path} we use an RRT algorithm to construct a path on the manifold. However the major difference with their work is how we construct a chart around a given point $\mathbf{x}_i$ and how we select a new candidate node for the RRT algorithm.

%Given a point $(\mathbf{x}_i, y_i\in\mathcal{S}^0$ such that $y_i=0$ we compute its estimated normal, $n_i$, using Eq.~\ref{eq:gpr_n} and the chart $\mathcal{C}_i$ as a disc centred in $\mathbf{x}_i$ with radius
%$$
%\rho=\frac{1}{\mathbb{V}[f(\mathbf{x}_i)]}
%$$
%which is inversely proportional to the expected variance of our model at point $\mathbf{x}_i$. This allows us to construct larger tangent space in region in which the confidence of the model is higher and thus to reduce the allowed exploring region when the model has less information.

%From the chart $\mathcal{C}_i$ we can randomly sample a finite set of $d$ candidate parameter vectors $\mathbf{u}_p^i$ with $p\in[1,\dots,d]$. Each parameter vector $\mathbf{u}_p^i$ encodes a direction of exploration from the central point $\mathbf{x}_i$ to a point $\mathbf{x}_p^i$ such that
%$$
%\|\mathbf{x}_i-\mathbf{x}_p^i\|_2=\rho
%$$
%laying on the circumference of the chart. Using the exponential map described in Sec.~\ref{sec:atlas}, we compute the projection $\mathbf{x}_p=\psi_i(\mathbf{u}_p^i))$ such that $\mathbb{E}[f(\mathbf{x}_p)]\approx0$ which leads us to points that are estimated to be on the object surface by our probabilistic model. An user-defined threshold $\epsilon$ can be set to modify the system of equations in Eq.~\ref{eq:projection} to
%\begin{align}
%\label{eq:approxprojection}
%\begin{cases}
%|f'(\mathbf{x}_p)|&\leq\epsilon \\
%\Phi_i^\top(\mathbf{x}_p-\mathbf{x}_p^i)&=0
%\end{cases}
%\end{align}
%where $f'(\mathbf{x}_p)=\mathbb{E}[f(\mathbf{x}_p)]$ and $|\cdot|$ is the absolute value function.

%We then select the direction $\mathbf{u}_{p'}^i$ such that
%$$
%p'=\argmax_{p\in[1,\dots,d]}{\mathbb{V}[f(\psi_i(\mathbf{u}_p^i))]}
%$$
%to obtained a candidate target point $\mathbf{x}_{p'}$ ion the manifold.



%Moreover, this technique has been succesfully combined with RRTs to compute path on a constrained configuration space, more details can be found in .


%The GP for implicit function defined in Sec.~\ref{sec:gpr} defines a probabilistic representation of a unknown surface and therefore a global parametrisation of such a surface is not available. However we can interpret the surface as a manifold, $\mathcal{F}$, implicitly defined by a set of constraints such that $\mathcal{F}=\{\mathbf{x}\in\mathbb{R}^n|f(\mathbf{x})=0\}$. The manifold is represented as a collection of $k$-dimensional parameter spaces called charts.
%Given a point $\mathbf{x}_i\in\mathbb{R}^n$ on the manifold, a chart $\mathcal{C}_i$ is defined as a local parametrisation of the orthogonal tangent space at $\mathbf{x}_i$. We denote $\mathbf{u}_p^i\in\mathcal{C}_i$ a parameter vector such that
%\begin{equation}
%\label{eq:psi}
%\mathbf{x}_p^i=\mathbf{x}_i+\Phi_i\mathbf{u}_p^i
%\mathbf{x}_j^i=\pmb{\phi}_i(\mathbf{u}_j^i)=\mathbf{x}_i+\Phi_i\mathbf{u}_j^i
%\end{equation}
%where $\Phi_i\in\mathbb{R}^{n\text{x}k}$ is an orthogonal basis for the tangent space to the manifold at $\mathbf{x}_i$ and $\mathbf{x}_p^i\in\mathbb{R}^n$ is a point laying on this tangent space reachable from $\mathbf{x}_i$ through the parameter $\mathbf{u}_p^i$. Notice that both points $\mathbf{x}_i$ and $\mathbf{x}_p^i$ are defined in $\mathbb{R}^n$; in the literature this $n$-dimensional space is sometime referred as the ambient space (here a 3D space) to distinguish it from the $k$-dimensional space of the manifold (2D), with $n > k > 0$.

%To compute the tangent basis, $\Phi_i$, we make use of the estimated normal, $\mathbf{n}_i=\mathbb{E}[f'(\mathbf{x})]$, obtained by approximating the gradient of the GP at the point $\mathbf{x}_i$, as described in Sec.~\ref{sec:gpr_der}. Then we estimate $\Phi_i$ such that it satisfies the following linear system
%$$
%\left[
%\begin{array}{c}
%\mathbf{n}_i \\
%\Phi_i^\top
%\end{array}\right]\Phi_i
%=
%\left[
%\begin{array}{c}
%0 \\
%I
%\end{array}\right]
%$$
%where $I$ identifies the $k\text{ x }k$ identity matrix.

%The chart $\mathcal{C}_i$ also defines a bijective map between parameters $\mathbf{u}_p^i\in\mathbb{R}^k$ and $n$-dimensional points on the manifold. The function $\psi_i(\mathbf{u}_p^i)=\mathbf{x}_p$ is known in the literature as the \emph{exponential map} and defines the relation between the parameter space of the chart $\mathcal{C}_i$ and any point on the manifold $\mathbf{x}_p$. Similarly, the \emph{logarithmic map} expresses the inverse relation, $\psi_i^{-1}(\mathbf{x}_p)= \mathbf{u}_p^i$.

%A typical implementation of the exponential map makes use of Eq.~\ref{eq:psi} to move along the tangent space at $\mathbf{x}_i$ in the direction defined by the parameter vector $\mathbf{u}_p^i$, then it projects the point $\mathbf{x}_p^i$ on the object surface by solving the system of equations
%\begin{align}
%\label{eq:projection}
%\begin{cases}
%f(\mathbf{x}_p)&=0\\
%\Phi_i^\top(\mathbf{x}_p-\mathbf{x}_p^i)&=0
%\end{cases}
%\end{align}
%to find a point $\mathbf{x}_p$ that lays on the manifold and, therefore, on the estimated object surface.

%---------------------------------------
%\subsubsection{Implicitly-defined manifold exploration}
%\label{sec:AtlasRRT}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Equipment specification and limitations}
\label{sec:limitations}

Tactile exploration requires a probe able to measure contact points on the surface of two rigid bodies, and preferably the normal at that contact point too. There are mainly two low-level sensor types suitable for this end: 1) tactile sensor arrays and 2) intrinsic tactile sensors\footnote{There are works reporting the use of a proprioceptive system, but we stick with these two.}. The first type is composed of a grid of pressure cells of fixed area, so the point resolution is limited to the quantization of the array. This kind of sensor has been widely used in the literature due to its multi-contact capability. However, in an scenario where a surface is not known precisely, it would require that the array be on top of a deformable body to exploit this feature most. But the deformation then becomes hard to measure, yielding useless point measures. Consequently, they are typically mounted on top of a quasi-rigid surface, also for safety of the cells, but in our scenario, we would be wasting the multi-contact capability. The second type is composed of a $6$-axis force-torque sensor and a known shape on top of it, typically one that allows the computation of the contact point and force in closed-form \cite{Bicchi1993Contact}. These are single-contact sensor with the resolution being that of the force-torque sensor measurement, which implicitly is the resolution of the A/D conversion of the internal load cells. This is, in general, smaller than the resolution of a tactile sensor array.

A consideration for both types is that, for tactile exploration, they need to be mounted on a robot with at least $6$ degrees of freedom, to allow the exploration to happen in different orientations w.r.t. the explored object. The mobility can be further increased if the object to be explored is also grasped by a movable robot with some degree of freedom. This inherently requires that the gripping device be soft and adaptive, but firm when grasping, since the initial assumption is that the shape is not known.

In both sensor types, the reachability space is of course limited to the size of the probe. It is worth considering, however, that with the intrinsic tactile sensor type, one can build a very small tip to reach tiny spaces.

% Object convexity concavity ?  Wait for Federico's feedback.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem definition}
\label{sec:problem_definition}

Given all these ingredients, particularly that of having a probabilistic way to represent the surface of an object with a GP over initial and partial observations, the problem is: can we devise an exploration strategy \emph{intrinsic} to the model that exploits its probabilistic nature to suggest points\footnote{Note that the points can be isolated (poking) or arranged in a path (sliding).} for tactile exploration such that the predicted shape from the model improves? The following section provides details of our proposed solution, the GPAtlasRRT strategy.

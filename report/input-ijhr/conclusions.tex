This paper presented a method for exploring an object on all sides with a finger. Key to our approach was a combined shape representation and planning algorithm (GPAtlasRRT). The planner allows the robot to estimate the location of unseen and untouched surface. Together they allow planning of local tactile exploration of an incompletely modelled object. We demonstrated the benefits both in simulation, and on a real robot. The real robot system also demonstrated the ability to grasp an object with one hand, segment this hand from the object in an initial point cloud, and then extend the model with touches guided by GPAtlasRRT.

This local tactile planning strategy required bringing together Gaussian process implicit surfaces and the determination of implicitly-defined manifolds via continuation techniques. This exploited the ability of Gaussian processes to naturally represent model uncertainty. 

The beneficial features of the approach are several. First, the planning method does not require the computation of the explicit form of the entire predicted shape. Second, the strategy makes no assumptions about the exploratory probe. Third, it can plan sequences of tactile actions across a contiguous portion of the object surface, thus providing a detailed surface reconstruction. Fourth, the robot implementation allows the robot to explore an object as it holds it.

The proposed strategy was compared to a naive one, where touch rays were directed randomly. Our strategy outperforms this, whether using a single touch, or a touch sequence. The strategy was also tested successfully using our Vito and Boris robots. Previous published methods simplified the setting by placing the object on a table and then planning actions in a Cartesian space. This does not permit the robot to plan to traverse to the unseen back surfaces of objects with a single finger, or to explore objects rotated as they held in the hand. Instead, by creating an object centred representation, the method presented here is able to handle these cases, which are characteristic of human tactile exploration of objects.

Several points deserve further attention and future work. Perhaps most relevant to this work is the consideration of gradient observations as described by \cite{Solak2003Derivative}, especially due to our hardware setup. This feature has been presented in related work but not previously exploited. Another interesting point arises when discussing how to locally explore the model, that is, the direction to move within a chart. A third interesting topic is the use of the proposed strategy to drive a control loop where the controller command can be part of, or even just the first single step, of the exploratory path. According to our experience this is feasible and promising road to explore. Implementation issues also lead to non-trivial scientific problems. For example, the problem of maintaining a stable grasp during bi-manual exploration needs to be more thoroughly addressed. We found that an underactuated hand could maintain a firm hold of the object, but movements of the object in hand could seriously degrade the model quality. This could be addressed by re-estimating the object position in hand by best fitting its pose against the parts of the model that are most certain. The problem can also be addressed by carefully controlling the applied forces during exploration. We believe that position based planners are inadequate for this task, and that various compliant/force control strategies could be applied.


% The incorporation of other exploration primitives is also an interesting topic, that is, in our case we returned a path, but also dynamic aspects in the form of time-parametrized path (a trajectory) can yield other kind of information that can leave to a better model.
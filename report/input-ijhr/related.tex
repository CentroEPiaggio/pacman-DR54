%\subsection{State of the art}
%\label{sec:SoA}

One of the first attempts to exploit active tactile exploration with passive stereo vision for object recognition was proposed by Allen~\cite{Allen1987Robotic}. In that paper, a rigid tactile sensor traced along the object's surface in pre-defined movements. The work was later extended to develop different exploratory procedures to acquire and interpret 3D information on the surface shape \cite{Allen1990Acquisition}. The exploratory procedures were, however, selected by a human. Single-finger tactile exploration strategies for recognizing polyhedral objects have also been evaluated in simulation \cite{Roberts1990ICRA,Caselli1996ICRA}. 

Multiple fingers have also been used for tactile sensing. Moll and Erdmann~\cite{Moll2003STAR} presented a method for reconstructing the shape and motion of an unknown convex object using three sensorized fingers. In that approach, the object's friction properties must be known a priori and its surface must be smooth, lacking sharp edges and corners.

%The early work by \cite{Allen1987Robotic} presents a hierarchical representation of the object. The tactile exploration strategy to refine is driven by local geometry features. This is engaged by using surface tracing algorithms. In that work, it is literally said: ``Given a starting and ending point on a surface, the sensor traces along the surface reporting its contact positions and normals as it moves along.'' However, no indication is given in how to determine those starting and ending points on the surface. \cite{Allen1990Acquisition} blah blah

Tactile sensing has been used for localisation rather than surface recovery. Petrovskaya and Khatib \cite{Petrovskaya2011Global} used tactile exploration to localize an object of known shape. Since full Bayesian estimation of the pose of a free body is computationally expensive they approximated the posterior with particles. For a well-constrained object dataset the approach performs pose estimation in under one second with high reliability.

Bayesian methods have also been employed in shape estimation. Meier et al.\cite{Meier2011Probabilistic} performed tactile shape reconstruction using a Kalman filter. Efficient Bayesian inference using Gaussian processes has been used by various authors. For example, Sommer et al. \cite{Sommer2014Bimanual} proposed a method for bimanual compliant tactile exploration that used the GP representation to smooth the noisy point data, although they did not exploit the GP representation to derive the exploratory strategy.

Dragiev et al. \cite{Dragiev2011Gaussian} presented one of the first works to employ the Gaussian Process Implicit Surface (GPIS) representation for concurrent representation of the object shape and guidance of grasping actions. However, that paper utilized only the maximum a posteriori (MAP) estimate of the shape and thus did not utilise the ability of the Gaussian process to capture the uncertainty in the surface estimate. Later work, by the same authors, \cite{Dragiev2013Uncertainty} offered a way to prefer regions of the model with a particular certainty level in their shape estimate and introduced the notion of explore-grasp and exploit-grasp primitives.

Algorithms for selecting the sequence of touches have been developed by various authors. Bierbaum et al. \cite{Bierbaum2008Potential} guided tactile exploration using Dynamic Potential Fields for motion guidance of the fingers. They showed that grasp affordances can be generated from geometric features extracted from the contact points.

Bjorkman et al. \cite{Bjorkman2013Enhancing} showed how to build object models with a small number of tactile actions (each action involving multiple simultaneous touches by several tactile arrays) with the aim of categorisation, rather than shape recovery. They employed the GPIS representation mentioned above, with the kernal used by the GP being the thin plate covariance function derived by Williams \cite{Williams2007Gaussian}. A set of predefined tactile glances are performed on the object: however, these are not updated as the object model is refined. 

The main difference is that our approach is what we term {\em local tactile exploration} rather than {\em global tactile exploration}. By this we mean that our touch choices are constrained by the Atlas-RRT process to be considered first at points close to the already explored surface. The area considered for exploration grows outwards until a suitably uncertain point is found. This local exploration is very different from a global exploration strategy, in which any touch action can be considered. There is no inherent benefit to either approach, but local exploration allows us to define a series of touches across a contiguous area of hypothesized surface. Local exploration is a strategy often employed by humans. Both local and global strategies are important, but this is the first paper in which a local exploration strategy is combined with a GP representation of shape uncertainty. 

There are also other smaller differences: the space in which the next-best exploratory action is computed, the grain size to compute the predicted shape, and the terminating condition for the overall algorithm.
%//, and the descriptor used. 
Bjorkman et al.
\cite{Bjorkman2013Enhancing}
% use Zernike moments which imply extra computation time, and lack of a probabilistic interpretation, which is one of advantages of using Gaussian Process in first place. Moreover,
% CR: we don't use the model in the end for anything, so we can't compare to the descriptors they use for discrimination
drew the exploratory actions from a discretisation of the vertical axis of the workspace and the approach angle. This works because the objects are placed upright on a table. But it means that the actions are extrinsic to the shape model. This is not suitable for exploration while the object is being held by the robot. Neither is there any guarantee that the contact will be on a particular location on the object surface. Moreover, since they are interested in a model that is useful for categorization, they propose Zernike moments to make it affine invariant, for which a fine-grained explicit representation is required. We also compute the predicted shape with a coarse grain for collision avoidance purposes, an issue that is not considered in that work. Finally, the number of touches in Bjorkman's work is subject to an absolute limit. The set is ordered according to {\bf the closest point on the implicit function with higher variance}. In contrast, we set a maximum aceptable uncertainty for the predicted shape. As a consequence we can continue to explore until the shape is sufficiently well known.

More recently, active touch using a GP model has been developed by Jamali et al. \cite{jamali2016}. That paper uses a combination of GP-regression and GP-classification to pick the next best sample point for a finger. This is driven to where the model has lowest confidence in its prediction. One difference with our approach is that the sampling points are specified in the $x,y$-plane and not in full 3D space. This somewhat simplifies their path planning problem, as they don't have to calculate a path over the surface. They also differ in that they do not employ an implicit surface model. In addition their system is uni-manual. 

A similar approach is that of Yi et al., who used a single finger probe to explore objects that are fixed to a surface. The next best point is the one that has the greatest variance in the height of the predicted object surface. This is similar to Jamali et al if the exploration were dominated by the GP-regression model. Our method differs from this work in a similar way to that in which it differs from Jamali et al. Our problem is to plan an exploration path on a 3D surface involving a sequence of contacts at a time, not to select a single next best touch point normal to a plane.

Finally, closer again to our approach is that of Matsubaru et al \cite{matsubaru2016}. In this a GPIS model is employed, together with a planner that accounts for the trade-off between the travel distance between touches, and the uncertainty in the surface at the proposed touch location. Thus, it is the first example of an active touch planner that is local while still being driven towards areas of uncertainty. The main restriction of that work is its restriction to a 2D model of the 3D object shape (its projection in the vertical plane), and the fact that exploration is again unimanual. The former allows the use of a grid-based discretization of the workspace as the space within which touches are chosen. In our work we instead focus on the problem of how to plan a path of touches for the robot across a 3D surface. To solve this we exploit RRT based planning in continuous but constrained configuration spaces, and approach which is arguably more scalable, although we do not make a comparison here.

% Matsubara et al. Active tactile exploration with uncertainty and travel cost for fast shape estimation of unknown objects
% Tosi et al. Action Selection for Touch-based Localisation Trading Off Information Gain and Execution Time
% Ottenhaus et al. Local Implicit Surface Estimation for Haptic Exploration
% N. Jamali, et al., "Active perception: Building objects' models using tactile exploration," 2016 IEEE-RAS 16th International Conference on Humanoid Robots (Humanoids), Cancun, 2016, pp. 179-185.
% Z. Yi et al., "Active tactile object exploration with Gaussian processes," 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, 2016, pp. 4925-4930.

%Comparing both that and this work is not easy due to the paradigmatical difference of the space in which the actions are searched. In their case, they benefit from several unprecise observations at once by tactile arrays, and in our case we focus on precise observations from an intrinsic tactile sensor. 
%\subsection{State of the art}
%\label{sec:SoA}

One of the first early attempts to exploit active tactile exploration with passive stereo vision for object recognition was proposed by \cite{Allen1987Robotic}. In that paper, a rigid finger-like tactile sensor was used to trace along the surface with predefined movement cycles and provided a limited amount of information on the object surface. The work was later extended to develop different exploratory procedures to acquire and interpret 3D touch information \cite{Allen1990Acquisition}. The exploratory procedures were, however, commanded by a human experimenter and therefore not linked to a fully autonomous system.

Single finger tactile exploration strategies for recognizing polyhedral objects have also been presented and evaluated in simulation, see for instance the work by \cite{Roberts1990ICRA} and \cite{Caselli1996ICRA}. In \cite{Moll2003STAR} a method for reconstructing shape and motion of an unknown convex object using three sensing fingers is presented. In that approach, friction properties must be known in advance and the surface is required to be smooth, i.e., it must have no corners or edges. Moreover, multiple simultaneous sensor contacts points are required, resulting in additional geometric constraints for the setup.

%The early work by \cite{Allen1987Robotic} presents a hierarchical representation of the object. The tactile exploration strategy to refine is driven by local geometry features. This is engaged by using surface tracing algorithms. In that work, it is literally said: ``Given a starting and ending point on a surface, the sensor traces along the surface reporting its contact positions and normals as it moves along.'' However, no indication is given in how to determine those starting and ending points on the surface. \cite{Allen1990Acquisition} blah blah

In the work of \cite{Petrovskaya2011Global}, exploratory procedures have been considered with the aim to globally localize an object of known shape. Since the Bayesian posterior estimation for objects in 6D is known to be computationally expensive, that paper proposes an efficient approach, termed Scaling Series, that approximates the posterior by particles. For well constrained datasets, that approach performs estimation in under 1 second with very high reliability.

In the paper presented by \cite{Meier2011Probabilistic} tactile shape reconstruction employs a Kalman filter, while in \cite{Bierbaum2008Potential} the tactile exploration is guided by Dynamic Potential Fields for motion guidance
of the fingers. Here, the authors show that grasp affordances may be generated from geometric features extracted from the contact point set extracted during tactile exploration.

Interestingly, \cite{Sommer2014Bimanual} proposes a method for bimanual compliant tactile exploration that uses the GP representation to smooth noisy point data, but does not exploit the GP representation to define specific exploratory strategies.

\cite{Dragiev2011Gaussian} presents one of the first works that employs Gaussian Process Implicit Surfaces (GPIS) for the concurrent representation of the object shape and to guide grasping actions towards the object. However, that work concentrates only on the mean of the shape distribution, i.e. the maximum a posteriori (MAP) estimate of the shape and ignores one of the potential benefits of the GP---the uncertainty in the estimate. Later work by the same authors \cite{Dragiev2013Uncertainty} offers a way to give preference to regions of the model with a particular certainty level and introduces the notion of explore-grasp and exploit-grasp primitives.

\cite{Bjorkman2013Enhancing} focuses on building object models with a small number of tactile actions (each action involving multiple simultaneous touches by several tactile arrays) with the aim of understanding the category an object belongs to, rather than trying to recover its full shape. In that paper, the implicit function representation of the object surface is modelled by Gaussian Process regression as well, where the shape of the GP is governed by the thin plate covariance function derived by \cite{Williams2007Gaussian}. A set of predefined tactile glances are performed on the object: however, these are not updated as the object model is refined. 

That paper is the closest work to ours among the references. The main difference is that our approach is what we term {\em local tactile exploration} rather than {\em global tactile exploration}. By this we mean that our touch choices are constrained by the Atlas-RRT process to be considered first at points close to the already explored surface. The area considered for exploration grows outwards until a suitably uncertain point is found. This local exploration is very different from a global exploration strategy, in which any touch action can be considered. There is no inherent benefit to either approach, but local exploration allows us to define a series of touches across a contiguous area of hypothesized surface. Local exploration is a strategy often employed by humans. Both local and global strategies are important, but this is the first paper in which a local exploration strategy is combined with a GP representation of shape uncertainty. 

There are also other smaller differences: the space in which the next-best exploratory action is computed, the grain size to compute the predicted shape, and the terminating condition for the overall algorithm.
%//, and the descriptor used.
\cite{Bjorkman2013Enhancing}
% use Zernike moments which imply extra computation time, and lack of a probabilistic interpretation, which is one of advantages of using Gaussian Process in first place. Moreover,
% CR: we don't use the model in the end for anything, so we can't compare to the descriptors they use for discrimination
proposes that the the exploratory actions are to be drawn from a discretisation of the vertical axis of the workspace and the approach angle. This works because the objects are placed upright on a table. But the considered exploration actions are extrinsic to the shape model. This is not suitable for exploration while the object is being held by the robot. Neither is there any guarantee that the contact will be on a desired location on the object surface. Moreover, since they are interested in a model that is useful for categorization, they propose Zernike moments to make it affine invariant, for which a fine-grained explicit representation is required. We also propose to compute the predicted shape but with a very coarse grain for collision avoidance purposes, an issue that is not considered in that work. Finally, the number of actions, or touches in that case, are limited to a certain number, and then ordered according to the closest point on the implicit function with higher variance. In contrast, we set the highest expected variance in the shape prediction, so we explore until, probabilistically speaking, that goal is achieved.
%Comparing both that and this work is not easy due to the paradigmatical difference of the space in which the actions are searched. In their case, they benefit from several unprecise observations at once by tactile arrays, and in our case we focus on precise observations from an intrinsic tactile sensor. 